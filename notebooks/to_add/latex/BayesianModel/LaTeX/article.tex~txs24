%%%%%%%%%%%%%%%%%%%%%%% file typeinst.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is the LaTeX source for the instructions to authors using
% the LaTeX document class 'llncs.cls' for contributions to
% the Lecture Notes in Computer Sciences series.
% http://www.springer.com/lncs       Springer Heidelberg 2006/05/04
%
% It may be used as a template for your own input - copy it
% to a new file with a new name and use it as the basis
% for your article.
%
% NB: the document class 'llncs' has its own and detailed documentation, see
% ftp://ftp.springer.de/data/pubftp/pub/tex/latex/llncs/latex2e/llncsdoc.pdf
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{listings}

\usepackage{tikz}
\usetikzlibrary{positioning,arrows,shapes.geometric,
	matrix,shapes.symbols,decorations.pathreplacing}

\usepackage{subcaption}
\captionsetup{compatibility=false}

\usepackage{booktabs}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{url}
\urldef{\mailsa}\path|loic_tetrel@yahoo.fr|
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
	\noindent\keywordname\enspace\ignorespaces#1}

\begin{document}
	
	\mainmatter  % start of an individual contribution
	
	% first the title is needed
	\title{Bayesian Model for Machine Learning}
	
	% a short form should be given in case it is too long for the running head
	\titlerunning{Bayesian Model for Machine Learning}%
	
	% the name(s) of the author(s) follow(s) next
	%
	% NB: Chinese authors should write their first names(s) in front of
	% their surnames. This ensures that the names appear correctly in
	% the running heads and the author index.
	%
	\author{Lo√Øc Tetrel}
	%
	\authorrunning{Bayesian Model for Machine Learning}
	% (feature abused for this document to repeat the title also on left hand pages)
	
	% the affiliations are given next; don't give your e-mail address
	% unless you accept that it will be published
	%\institute{******************************************\\
	\institute{}
	
	%
	% NB: a more complex sample for affiliations and the mapping to the
	% corresponding authors can be found in the file "llncs.dem"
	% (search for the string "\mainmatter" where a contribution starts).
	% "llncs.dem" accompanies the document class "llncs.cls".
	%
	
	\toctitle{Bayesian Model for Machine Learning}
	
	\maketitle

	\keywords{Generative model; Bayes; Prediction}

	\section{Introduction}\label{introduction}
	Thomas Bayes is a well known mathematician, with work focused in probability. Bayes theory\cite{bayes1958essay} is central in machine learning and has numerous applications in automatic classification.
	
	\section{Theory}\label{theory}
	Consider two event $a$ and $b$, the Bayesian rule define the probability $P$ that $a$ occurs, knowing the result of the event $b$ :
	
	\begin{equation}
	P(a|b) = \frac{P(b|a) \times P(a)}{P(b)} 
	\end{equation}
	
	It can be interpreted as is : if we know a prior the probabilities of the events $a$, $b$, and the result of the event $a$ (knowing result of $b$), then we can deduct the result $b$ (knowing result of $a$). \\ Using this rule will allow us to infer things about the income of events, when you have some aprioris. This is the base in Machine Learning, with training data, you infer the result for testing data.
	
	
	In Machine learning, we usually refer the probability as a function.
		
	\section{Code example}\label{example}
	
	Simple example with two gaussians from apple and strawberry and radius
	
	Below you can see the effect on the $\mathcal{GP}$ when some inputs comes.
	You may notice that the outputs from the prior (vertical axis) are coming from a gaussian distribution :
	
	\begin{figure}
		\centering
		\includegraphics[width=0.5\textwidth]{Figures/GPprior}
		\\ \parbox{0.75\textwidth}{\caption[Gaussian Process prior]{Three random function taken from the prior. The mean function (red) is zero, and the covariance function constant.}\label{fig:GPprior}} 
	\end{figure}
	
	\begin{figure}
		\centering
		\includegraphics[width=0.5\textwidth]{Figures/GPposterior}
		\\ \parbox{0.75\textwidth}{\caption[Gaussian Process posterior]{With 5 test data (blue squares), three different samples are taken from the posterior. We can now evaluate the mean function (red) and the covariance at 95\% (grey). }\label{fig:GPposterior}} 
	\end{figure}
	
	To generate the figure, you can use this code (slightly modified from \url{http://katbailey.github.io/post/gaussian-processes-for-dummies/}):
	
	\lstinputlisting[language=python]{PriorPosterior.py}
	
	\section{Study case}
	\label{sec:method}
	
	\section{Conclusion}
	\label{sec:conclusion}
	
	 
	\subsection*{Acknowledgment}
	Thanks
	
	{\small
		\bibliographystyle{splncs03}
		\bibliography{article}
	}
	
\end{document}

