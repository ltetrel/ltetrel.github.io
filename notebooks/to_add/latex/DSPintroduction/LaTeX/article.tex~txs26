%%%%%%%%%%%%%%%%%%%%%%% file typeinst.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is the LaTeX source for the instructions to authors using
% the LaTeX document class 'llncs.cls' for contributions to
% the Lecture Notes in Computer Sciences series.
% http://www.springer.com/lncs       Springer Heidelberg 2006/05/04
%
% It may be used as a template for your own input - copy it
% to a new file with a new name and use it as the basis
% for your article.
%
% NB: the document class 'llncs' has its own and detailed documentation, see
% ftp://ftp.springer.de/data/pubftp/pub/tex/latex/llncs/latex2e/llncsdoc.pdf
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{listings}

\usepackage{tikz}
\usetikzlibrary{positioning,arrows,shapes.geometric,
	matrix,shapes.symbols,decorations.pathreplacing}

\usepackage{subcaption}
\captionsetup{compatibility=false}

\usepackage{booktabs}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{url}
\urldef{\mailsa}\path|loic_tetrel@yahoo.fr|
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
	\noindent\keywordname\enspace\ignorespaces#1}

\begin{document}
	
	\mainmatter  % start of an individual contribution
	
	% first the title is needed
	\title{Bayesian Model for Machine Learning}
	
	% a short form should be given in case it is too long for the running head
	\titlerunning{Bayesian Model for Machine Learning}%
	
	% the name(s) of the author(s) follow(s) next
	%
	% NB: Chinese authors should write their first names(s) in front of
	% their surnames. This ensures that the names appear correctly in
	% the running heads and the author index.
	%
	\author{Lo√Øc Tetrel}
	%
	\authorrunning{Bayesian Model for Machine Learning}
	% (feature abused for this document to repeat the title also on left hand pages)
	
	% the affiliations are given next; don't give your e-mail address
	% unless you accept that it will be published
	%\institute{******************************************\\
	\institute{}
	
	%
	% NB: a more complex sample for affiliations and the mapping to the
	% corresponding authors can be found in the file "llncs.dem"
	% (search for the string "\mainmatter" where a contribution starts).
	% "llncs.dem" accompanies the document class "llncs.cls".
	%
	
	\toctitle{Bayesian Model for Machine Learning}
	
	\maketitle

	\keywords{Generative model; Bayes; Prediction}

	\section{Introduction}\label{introduction}
	Thomas Bayes is a well known mathematician, with work focused in probability. Bayes theory\cite{bayes1958essay} is central in machine learning and has numerous applications in automatic classification.
	
	\section{Theory}\label{theory}
	Consider two event $a$ and $b$, the Bayesian rule define the probability $P$ that $a$ occurs, knowing the result of the event $b$ :
	
	\begin{equation}
	P(a|b) = \frac{P(b|a) \times P(a)}{P(b)} 
	\end{equation}
	
	It can be interpreted as is : if we know a priori the probabilities of the events $a$, $b$, and the result of the event $b$ (knowing result of $a$), then we can deduct the result $a$ (knowing result of $b$). \\ Using this rule will allow us to infer things about the income of events, when you have some aprioris. This is the base in Machine Learning, with training data, you infer the result for testing data.
		
	\section{Code example}\label{example}
	Let's do a pratical example of what Bayes is. Imagine you are in charge of a production line, this line sort strawberries from grapes. To do this, you bought cameras and measure the radius of the fruits coming from the farmers. Your hypothesis is that the radius is a good predictor, and you want to use a bayesian rule to classify the fruits !
	
	\begin{equation}
	P(\omega_i|X) = \frac{P(X|\omega_i) \times P(\omega_i)}{P(X)} 
	\end{equation}
	
	Where $\omega_i = {\omega_g, \omega_c}$ are the events for $\omega_g$ : "The fruit is a grape", and $\omega_c$ "The fruit is a cherry". The measured radius is called $X$.
	
	\par
	First, you want to model the distribution of your fruits. In Machine learning, we usually refer the probability as a function called pdf (probability density function). A gaussian prior is often used. So you start acquiring many data while knowing what fruit it is, and calculate the parameters ($\mu$, $\sigma$) of the associated gaussian \footnote{Usually you want to calculate the likelihood of your data to verify if it's truly gaussian.}.
	
	\begin{lstlisting}[language=python]
	# Creation of data
	RadiusCherries = np.random.normal(4, 0.5, 1000)
	RadiusGrapes = np.random.normal(2, 0.5, 1000)
	
	# Estimators for gaussian mean and deviation
	MeanCherries = np.median(RadiusCherries)
	DevCherries = np.std(RadiusCherries)
	MeanGrapes = np.median(RadiusGrapes)
	DevGrapes = np.std(RadiusGrapes)
	\end{lstlisting}
	
	Definition with claa and predictor
	Hypothesis of normal data (which normally needs to be confirmed by a Student test by the way...)
	Simple example with two gaussians from apple and strawberry and radius
	
	It is als possible t evaluate the probability if we introduce the noise behind the detector. We can use the mahalanobis distance between the model and the measure. This is called a generative model, because instaed of giving a prediction, we give a level of uncertainty
	
	\lstinputlisting[language=python]{PriorPosterior.py}
	
	\section{Study case}
	\label{sec:method}
	
	\section{Conclusion}
	\label{sec:conclusion}
	Example of Monty Hall problem, resolved using bayesian rule :
	\url{https://www.quora.com/How-do-I-solve-the-Monty-Hall-Problem-using-Bayes-Theorem}
	 
	\subsection*{Acknowledgment}
	Thanks
	
	{\small
		\bibliographystyle{splncs03}
		\bibliography{article}
	}
	
\end{document}

